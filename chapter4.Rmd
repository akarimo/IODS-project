# Chapter 4: Clustering and classification

First, I load the data and look at the structure and dimensions. The dataset includes data on per capita crime rate by town (crim), and different variables regardin the residents, homes, and businesses in these towns. The data has 506 observations and 14 variables.

```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the structure and dimensions of the dataset
str(Boston)
dim(Boston)

```

In the plot below we can see how these variables correlate with each other. Negative correlations are marked red, and positive correlations with blue. We can see that there are quite strong positive correlations for example between proportion of non-retail business acres per town (indus) and nitrogen oxides concentration (nox), as well as between index of accessibility to radial highways (rad) and full-value property-tax rate per $10,000 (tax). There are strong negative correlations between weighted mean of distances to five Boston employment centres (dis) and indus, nox, and proportion of owner-occupied units built prior to 1940 (age).

Summaries of these variables show that the scales of these variables vary a lot. Some variables range between 0 and 1 (e.g. chas), while others range between 187 and 711 (tax).

```{r}
library(corrplot)
library(tidyverse)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 

#save rounded matrix
cor_matrix <- cor_matrix %>% round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

#summaries of the variables
summary(Boston)

```

To make the variables more comparable I standardize them. Below we can see that the scaled variables are now centered around zero.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

Next, I create a categorical variable of the crime rate.

```{r}
# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

And divide the dataset to train and test sets, so that 80% of the data belongs to the train set

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

Next, I fit a linear discriminant analysis on the training dataset. In the plot below we see the results of the analysis, where the observatios are coloured according to the categorical crime rate.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

lda.arrows <- function(x, myscale = 1, arrow.heads = 0.1, color = 'purple', tex = 0.75, choices = c(1,2)) {
  heads = coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale*heads[,choices[1]],
         y1 = myscale*heads[,choices[2]],
         col = color, length = arrow.heads)
  text(myscale*heads[,choices], labels = row.names(heads), cex = tex, col = color, pos = 3)
}
classes = as.numeric(train$crime)

#plot the result
plot(lda.fit, dimen = 2, col = classes)
lda.arrows(lda.fit, myscale = 2)

```

I save the correct crime rate classes from the data.

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

And predict the classes with the test data. In the table below we see that the "high" category is predicted perfectly, but more errors are made with the other categories. For example from the "med_high" category, only 14 observations are predicted to be in the correct category, while 14 are predicted to be in the "med_low" category, and 3 in the "high" category.

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Next, I reload the dataset, scale the variables again, and look at euclidean distance between observations.

```{r}

# reload the data
data(Boston)
# center and standardize variables
boston_scaled <- scale(Boston)
boston_scaled <- data.frame(boston_scaled)

# euclidean distance matrix
eu_dist <- dist(boston_scaled)
```

Before running a k-means algorithm, I investigate what is the optimal number of clusters. In the plot below we see, that the optimal number of clusters is 2, because the total WCSS drops radically.

```{r}
set.seed(123) #set seed to ensure results are replicable

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results

library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Next I run the k-means algorithm with 2 clusters and plot the clusters. In the plot we see that the clusters seem to differ especially regarding the variables indus, nox, age, dis, rad, tax, ptratio, and lstat.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the scaled Boston dataset with clusters
library(GGally)
ggpairs(boston_scaled, mapping = aes(col = as.factor(km$cluster), alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

```

